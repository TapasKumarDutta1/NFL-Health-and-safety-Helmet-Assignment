{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_auc_with_uid_embedding_swa",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_auc_with_uid_embedding_swa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8a4844-053c-4854-f09f-6290e3eb8018"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 109MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 161MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 77% 45.0M/58.3M [00:00<00:00, 122MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 168MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 69% 36.0M/52.2M [00:00<00:00, 116MB/s] \n",
            "100% 52.2M/52.2M [00:00<00:00, 207MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 215MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612dc615-c3ed-4d62-ca81-d36cdf7288c2"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "389cf02d-fa81-466a-fbda-c71064764165"
      },
      "source": [
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train_id.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test_id.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C9_std_isna</th>\n",
              "      <th>C10_std_isna</th>\n",
              "      <th>C11_std_isna</th>\n",
              "      <th>C12_std_isna</th>\n",
              "      <th>C13_std_isna</th>\n",
              "      <th>C14_std_isna</th>\n",
              "      <th>D1_mean_isna</th>\n",
              "      <th>D1_std_isna</th>\n",
              "      <th>D2_mean_isna</th>\n",
              "      <th>D2_std_isna</th>\n",
              "      <th>D3_mean_isna</th>\n",
              "      <th>D3_std_isna</th>\n",
              "      <th>D4_mean_isna</th>\n",
              "      <th>D4_std_isna</th>\n",
              "      <th>D5_mean_isna</th>\n",
              "      <th>D5_std_isna</th>\n",
              "      <th>D6_mean_isna</th>\n",
              "      <th>D6_std_isna</th>\n",
              "      <th>D7_mean_isna</th>\n",
              "      <th>D7_std_isna</th>\n",
              "      <th>D8_mean_isna</th>\n",
              "      <th>D8_std_isna</th>\n",
              "      <th>D9_mean_isna</th>\n",
              "      <th>D9_std_isna</th>\n",
              "      <th>D10_mean_isna</th>\n",
              "      <th>D10_std_isna</th>\n",
              "      <th>D11_mean_isna</th>\n",
              "      <th>D11_std_isna</th>\n",
              "      <th>D12_mean_isna</th>\n",
              "      <th>D12_std_isna</th>\n",
              "      <th>D13_mean_isna</th>\n",
              "      <th>D13_std_isna</th>\n",
              "      <th>D14_mean_isna</th>\n",
              "      <th>D14_std_isna</th>\n",
              "      <th>D15_mean_isna</th>\n",
              "      <th>D15_std_isna</th>\n",
              "      <th>V1_mean_isna</th>\n",
              "      <th>V1_std_isna</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wnan315.013926-13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wgmail.com325.027551.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Woutlook.com330.046631.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wyahoo.com476.018132-111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Hgmail.com420.044971.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 619 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_std_isna  isFraud                          id\n",
              "0  1.0  0.0  0.0  ...            1        0         Wnan315.013926-13.0\n",
              "1  1.0  0.0  0.0  ...            1        0      Wgmail.com325.027551.0\n",
              "2  1.0  0.0  0.0  ...            0        0    Woutlook.com330.046631.0\n",
              "3  1.0  0.0  0.0  ...            1        0  Wyahoo.com476.018132-111.0\n",
              "4  0.0  0.0  0.0  ...            1        0      Hgmail.com420.044971.0\n",
              "\n",
              "[5 rows x 619 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33635f7-2584-4f66-843c-965267c3d955"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2793.39 MB\n",
            "Memory usage after optimization is: 678.37 MB\n",
            "Decreased by 75.7%\n",
            "Memory usage of dataframe is 2392.90 MB\n",
            "Memory usage after optimization is: 580.09 MB\n",
            "Decreased by 75.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ"
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW"
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data,epochs):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "        self.epochs=epochs\n",
        "        self.val=0\n",
        "        self.wts=[]\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        if roc_val>self.val:\n",
        "          self.val=roc_val\n",
        "          self.epoch=10\n",
        "          self.wts=self.model.get_weights()\n",
        "        else:\n",
        "          self.epoch-=1\n",
        "        if self.epoch==0:\n",
        "          self.model.set_weights(self.wts)\n",
        "          self.model.stop_training = True\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model(dim):\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "  uid=Input((1,))\n",
        "  inp=Input((873,))\n",
        "  emb=Embedding(input_dim=dim,output_dim=4)(uid)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Concatenate()([emb,x])\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp,uid],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZH6xEokzPfj"
      },
      "source": [
        "Adding all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZw0LXIzPG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9143e2-c7e6-425c-d9d3-72afc44c008d"
      },
      "source": [
        "trn_s=trn.shape[0]\n",
        "df=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "autoenc=pd.read_csv('/content/gdrive/My Drive/fraud/without_id.csv',index_col=[0])\n",
        "\n",
        "autoenc.columns=[i for i in range(444,444+autoenc.shape[1])]\n",
        "\n",
        "\n",
        "df=pd.concat([df,autoenc],1)\n",
        "df=reduce_mem_usage(df)\n",
        "del([autoenc])\n",
        "gc.collect()\n",
        "\n",
        "trn=df.loc[:trn_s-1]\n",
        "tst=df.loc[trn_s:].reset_index(drop=True)\n",
        "del([df])\n",
        "gc.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 3384.06 MB\n",
            "Memory usage after optimization is: 1783.79 MB\n",
            "Decreased by 47.3%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxmq8JSOz6Hk"
      },
      "source": [
        "categorical=[str(i) for i in range(444)]\n",
        "trn[categorical]=trn[categorical].astype('uint8')\n",
        "tst[categorical]=tst[categorical].astype('uint8')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVXGvLePKR8T"
      },
      "source": [
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 1.5\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w55g3o_D_eYn"
      },
      "source": [
        "class stocasticensembling(Callback):\r\n",
        "  def __init__(self,model_name,alpha1,alpha2,iter_per_epoch,cycle_len,seqs_dict,start_inx=0,save_se_weights=False,folder='/content',**kwargs):\r\n",
        "    #save_se_weights: save after each epoch ?\r\n",
        "\r\n",
        "    super(stocasticensembling,self).__init__()\r\n",
        "    self.model_count=0\r\n",
        "    self.alpha1=alpha1\r\n",
        "    self.alpha2=alpha2\r\n",
        "    self.clr_iterations=0\r\n",
        "    self.cycle_num=cycle_len\r\n",
        "    self.cycle_len=cycle_len\r\n",
        "    self.iter_per_epoch=iter_per_epoch\r\n",
        "    self.iter_per_cycle = self.cycle_len * self.iter_per_epoch\r\n",
        "    self.save_se_weights=save_se_weights\r\n",
        "    self.start_inx=start_inx\r\n",
        "    self.swa_weights=[]\r\n",
        "    self.folder=folder\r\n",
        "    self.seqs_dict=seqs_dict\r\n",
        "    self.model_name=model_name\r\n",
        "    self.prob_dict={k: [] for k in self.seqs_dict.keys()}\r\n",
        "    self.lrs=[]\r\n",
        "\r\n",
        "  def on_train_end(self,logs={}):\r\n",
        "    self.weight_update()\r\n",
        "    self.model.set_weights(self.swa_weights)\r\n",
        "    self.snapsort()\r\n",
        "    for seq_names,probs in self.prob_dict.items():\r\n",
        "      self.prob_dict[seq_names]=np.concatenate(probs,axis=-1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_epoch_begin(self,epoch,logs=None):\r\n",
        "    self.current_epoch=epoch\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_epoch_end(self,epoch,logs=None):\r\n",
        "    self.cycle_num+=1\r\n",
        "    if (self._t_cycle() !=1) or (epoch == 15):\r\n",
        "      return\r\n",
        "    self.snapsort()\r\n",
        "    self.weight_update()\r\n",
        "    self.model_count+=1\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_batch_begin(self,batch,logs=None):\r\n",
        "    self.clr_iterations+=1\r\n",
        "    lr=self._clr_schedule()\r\n",
        "    self.lrs.append(lr)\r\n",
        "    K.set_value(self.model.optimizer.lr,lr)\r\n",
        "  \r\n",
        "  \r\n",
        "  def snapsort(self):\r\n",
        "    print(self.clr_iterations)\r\n",
        "    print(K.eval(self.model.optimizer.lr))\r\n",
        "    for seq_name,seq in self.seqs_dict.items():\r\n",
        "      self.prob_dict[seq_name].append(self.model.predict(seq,steps=len(seq)))\r\n",
        "  \r\n",
        "  \r\n",
        "  def weight_update(self):\r\n",
        "    weights=self.model.get_weights()\r\n",
        "    if self.model_count==0:\r\n",
        "      self.swa_weights=weights\r\n",
        "    for i in range(0,len(weights)):\r\n",
        "      self.swa_weights[i]=(self.swa_weights[i]*self.model_count+weights[i])/(self.model_count+1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def _t_cycle(self):\r\n",
        "        return (((self.clr_iterations - 1) % self.iter_per_cycle) + 1) / self.iter_per_cycle\r\n",
        "  \r\n",
        "  \r\n",
        "  def _clr_schedule(self):\r\n",
        "    return ((1.0 - 1.0 *self._t_cycle()) * self.alpha2) + (1.0 *self._t_cycle() *self.alpha1)\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60065e1-c7e0-4d46-e39e-2b2462f843eb"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from matplotlib import pyplot as plt\n",
        "splits=KFold(n_splits=5)\n",
        "gc.collect()\n",
        "pre=np.zeros((506691,1))\n",
        "# tst=tst.drop(['isFraud'],1)\n",
        "for train_index,test_index in tqdm(splits.split(trn)):\n",
        "  X_train, X_test = trn.loc[train_index], trn.loc[test_index]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  ids={}\n",
        "  for en,id in enumerate(X_train['id'].unique()):\n",
        "    ids[id]=en+2\n",
        "  X_train['id']=X_train['id'].map(lambda x: ids.get(x,1))\n",
        "  X_test['id']=X_test['id'].map(lambda x: ids.get(x,1))\n",
        "  dim=X_train['id'].nunique()+2\n",
        "  gc.collect()\n",
        "  trn_id,tst_id=X_train['id'],X_test['id']\n",
        "  X_train=X_train.drop(['isFraud','id'],1)\n",
        "  X_test=X_test.drop(['isFraud','id'],1)\n",
        "  mod=load_model(dim)\n",
        "  roc = RocCallback(validation_data=([X_test,tst_id], y_test),epochs=10)\n",
        "  mod.compile(optimizer=Nadam(),loss=rac)\n",
        "  es=EarlyStopping(monitor='acu_val',min_delta=0.0001,mode='min',restore_best_weights=True,patience=10)\n",
        "  seqs_dict={'test':[tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))]}\n",
        "  se = stocasticensembling(seqs_dict=seqs_dict, cycle_len=4, iter_per_epoch=231,\n",
        "                                  alpha1=5e-4, alpha2=5e-3,\n",
        "                                   model_name=\"model\", verbose=1)\n",
        "  mod.fit([X_train,trn_id],y_train,validation_data=([X_test,tst_id],y_test),batch_size=2048,epochs=16,callbacks=[se])\n",
        "  \n",
        "  del[(X_train,y_train)]\n",
        "  gc.collect()\n",
        "\n",
        "  mod.fit([X_test,tst_id],y_test,epochs=2,batch_size=2048)\n",
        "  pre+=se.prob_dict['test'].mean(1).reshape(506691,1)\n",
        "  pre+=mod.predict([tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))])\n",
        "\n",
        "  del([X_test,y_test,mod])\n",
        "  gc.collect()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "231/231 [==============================] - 4s 12ms/step - loss: 11305.0837 - val_loss: 4389.0977\n",
            "Epoch 2/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 1701.8008 - val_loss: 4126.0117\n",
            "Epoch 3/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 364.9895 - val_loss: 4258.5200\n",
            "Epoch 4/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 192.2189 - val_loss: 4199.4307\n",
            "924\n",
            "0.0005\n",
            "Epoch 5/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 162.8507 - val_loss: 4151.2031\n",
            "Epoch 6/16\n",
            "231/231 [==============================] - 2s 8ms/step - loss: 103.4859 - val_loss: 4043.5806\n",
            "Epoch 7/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 70.2694 - val_loss: 3945.2969\n",
            "Epoch 8/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 55.8544 - val_loss: 3984.4700\n",
            "1848\n",
            "0.0005\n",
            "Epoch 9/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 68.6286 - val_loss: 3976.6179\n",
            "Epoch 10/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 59.8819 - val_loss: 3530.9031\n",
            "Epoch 11/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 50.6427 - val_loss: 3724.8818\n",
            "Epoch 12/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 37.9758 - val_loss: 3679.9363\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/16\n",
            "231/231 [==============================] - 2s 8ms/step - loss: 43.0666 - val_loss: 3733.9089\n",
            "Epoch 14/16\n",
            "231/231 [==============================] - 2s 8ms/step - loss: 39.3061 - val_loss: 3599.5315\n",
            "Epoch 15/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 34.0060 - val_loss: 3435.0757\n",
            "Epoch 16/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 28.3755 - val_loss: 3632.2546\n",
            "3696\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 2805.1321\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 2172.9084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1it [01:12, 72.34s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "231/231 [==============================] - 4s 12ms/step - loss: 10579.9011 - val_loss: 6227.7539\n",
            "Epoch 2/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 1789.2496 - val_loss: 4982.3081\n",
            "Epoch 3/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 345.0141 - val_loss: 5629.7695\n",
            "Epoch 4/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 164.8317 - val_loss: 5461.3691\n",
            "924\n",
            "0.0005\n",
            "Epoch 5/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 151.8885 - val_loss: 5047.3457\n",
            "Epoch 6/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 85.1017 - val_loss: 5235.7168\n",
            "Epoch 7/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 63.4835 - val_loss: 5079.3706\n",
            "Epoch 8/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 53.9297 - val_loss: 4996.7876\n",
            "1848\n",
            "0.0005\n",
            "Epoch 9/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 58.7891 - val_loss: 4829.8389\n",
            "Epoch 10/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 51.1623 - val_loss: 4787.8438\n",
            "Epoch 11/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 43.9061 - val_loss: 4805.9058\n",
            "Epoch 12/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 35.1712 - val_loss: 4876.2822\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 41.5998 - val_loss: 4469.9785\n",
            "Epoch 14/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 39.0980 - val_loss: 4731.7607\n",
            "Epoch 15/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 29.8898 - val_loss: 4435.6343\n",
            "Epoch 16/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 26.1258 - val_loss: 4635.3984\n",
            "3696\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 3349.3628\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 2593.8489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2it [02:24, 72.33s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "231/231 [==============================] - 4s 12ms/step - loss: 10462.0594 - val_loss: 5743.9556\n",
            "Epoch 2/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 1990.6090 - val_loss: 4763.4941\n",
            "Epoch 3/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 391.8121 - val_loss: 5226.7988\n",
            "Epoch 4/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 183.7026 - val_loss: 5341.2910\n",
            "924\n",
            "0.0005\n",
            "Epoch 5/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 166.5191 - val_loss: 4981.4253\n",
            "Epoch 6/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 90.1942 - val_loss: 4795.3931\n",
            "Epoch 7/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 70.0593 - val_loss: 4802.5649\n",
            "Epoch 8/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 55.6326 - val_loss: 4770.0737\n",
            "1848\n",
            "0.0005\n",
            "Epoch 9/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 61.8648 - val_loss: 4577.2905\n",
            "Epoch 10/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 50.5896 - val_loss: 4633.9355\n",
            "Epoch 11/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 40.6142 - val_loss: 4436.7280\n",
            "Epoch 12/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 35.0698 - val_loss: 4535.7046\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 39.9564 - val_loss: 4049.2056\n",
            "Epoch 14/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 37.3223 - val_loss: 4309.6465\n",
            "Epoch 15/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 32.8276 - val_loss: 4400.4253\n",
            "Epoch 16/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 27.7969 - val_loss: 4352.4805\n",
            "3696\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 3277.7854\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2541.7429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3it [03:37, 72.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "231/231 [==============================] - 4s 13ms/step - loss: 10739.3659 - val_loss: 4975.2529\n",
            "Epoch 2/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 1967.5490 - val_loss: 4122.8042\n",
            "Epoch 3/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 410.7649 - val_loss: 4383.6382\n",
            "Epoch 4/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 195.0613 - val_loss: 4358.8477\n",
            "924\n",
            "0.0005\n",
            "Epoch 5/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 173.6109 - val_loss: 4143.9443\n",
            "Epoch 6/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 119.6434 - val_loss: 4445.6318\n",
            "Epoch 7/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 68.7753 - val_loss: 4002.3347\n",
            "Epoch 8/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 53.4789 - val_loss: 3915.1516\n",
            "1848\n",
            "0.0005\n",
            "Epoch 9/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 64.3269 - val_loss: 3947.1665\n",
            "Epoch 10/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 56.5011 - val_loss: 3718.0281\n",
            "Epoch 11/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 44.1903 - val_loss: 3901.9670\n",
            "Epoch 12/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 38.5196 - val_loss: 3800.0122\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 39.5840 - val_loss: 3723.3767\n",
            "Epoch 14/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 36.7708 - val_loss: 3499.2593\n",
            "Epoch 15/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 33.1516 - val_loss: 3719.2175\n",
            "Epoch 16/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 28.0677 - val_loss: 3689.2261\n",
            "3696\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2880.9485\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2301.4937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "4it [04:50, 72.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "231/231 [==============================] - 4s 12ms/step - loss: 9856.4982 - val_loss: 5967.0962\n",
            "Epoch 2/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 1982.3349 - val_loss: 4728.1538\n",
            "Epoch 3/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 363.6245 - val_loss: 5426.9780\n",
            "Epoch 4/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 169.1307 - val_loss: 5447.5527\n",
            "924\n",
            "0.0005\n",
            "Epoch 5/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 161.4387 - val_loss: 4922.8306\n",
            "Epoch 6/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 99.0840 - val_loss: 4701.5986\n",
            "Epoch 7/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 71.8682 - val_loss: 4822.4424\n",
            "Epoch 8/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 61.4692 - val_loss: 4937.4756\n",
            "1848\n",
            "0.0005\n",
            "Epoch 9/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 97.7951 - val_loss: 5008.5063\n",
            "Epoch 10/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 110.3913 - val_loss: 4592.3594\n",
            "Epoch 11/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 44.7024 - val_loss: 4425.6313\n",
            "Epoch 12/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 38.2195 - val_loss: 4355.9355\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 45.9907 - val_loss: 4236.2515\n",
            "Epoch 14/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 49.0758 - val_loss: 4434.6567\n",
            "Epoch 15/16\n",
            "231/231 [==============================] - 2s 9ms/step - loss: 36.0939 - val_loss: 4434.3750\n",
            "Epoch 16/16\n",
            "231/231 [==============================] - 2s 8ms/step - loss: 29.4890 - val_loss: 4367.1504\n",
            "3696\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 3510.0107\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2776.5254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5it [06:02, 72.57s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL1HvlFBHDyP",
        "outputId": "d2f18625-ae1e-4e28-b681-7e8141266645"
      },
      "source": [
        "np.max(pre)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.999362289905548"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "11b08015-5269-479d-c656-0c0b98bf8932"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre/10\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.072171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.107650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.228729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.211140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.301400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.072171\n",
              "3663550        0.107650\n",
              "3663551        0.228729\n",
              "3663552        0.211140\n",
              "3663553        0.301400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo"
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "fc4ec2b8-7bd4-4531-a012-72b4b1921af5"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre/10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c0e538780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MroLdHR2HkBH"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}
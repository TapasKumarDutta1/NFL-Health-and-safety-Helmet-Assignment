{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_with_uid_embedding",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_with_uid_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0454e329-5cb7-4880-bae2-c148d712f078"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c16d3e3-9362-4edc-a069-0f8df8e44d80"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "5a6f6a8b-80e9-463a-f816-3514ede26b3b"
      },
      "source": [
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train_id.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test_id.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C9_std_isna</th>\n",
              "      <th>C10_std_isna</th>\n",
              "      <th>C11_std_isna</th>\n",
              "      <th>C12_std_isna</th>\n",
              "      <th>C13_std_isna</th>\n",
              "      <th>C14_std_isna</th>\n",
              "      <th>D1_mean_isna</th>\n",
              "      <th>D1_std_isna</th>\n",
              "      <th>D2_mean_isna</th>\n",
              "      <th>D2_std_isna</th>\n",
              "      <th>D3_mean_isna</th>\n",
              "      <th>D3_std_isna</th>\n",
              "      <th>D4_mean_isna</th>\n",
              "      <th>D4_std_isna</th>\n",
              "      <th>D5_mean_isna</th>\n",
              "      <th>D5_std_isna</th>\n",
              "      <th>D6_mean_isna</th>\n",
              "      <th>D6_std_isna</th>\n",
              "      <th>D7_mean_isna</th>\n",
              "      <th>D7_std_isna</th>\n",
              "      <th>D8_mean_isna</th>\n",
              "      <th>D8_std_isna</th>\n",
              "      <th>D9_mean_isna</th>\n",
              "      <th>D9_std_isna</th>\n",
              "      <th>D10_mean_isna</th>\n",
              "      <th>D10_std_isna</th>\n",
              "      <th>D11_mean_isna</th>\n",
              "      <th>D11_std_isna</th>\n",
              "      <th>D12_mean_isna</th>\n",
              "      <th>D12_std_isna</th>\n",
              "      <th>D13_mean_isna</th>\n",
              "      <th>D13_std_isna</th>\n",
              "      <th>D14_mean_isna</th>\n",
              "      <th>D14_std_isna</th>\n",
              "      <th>D15_mean_isna</th>\n",
              "      <th>D15_std_isna</th>\n",
              "      <th>V1_mean_isna</th>\n",
              "      <th>V1_std_isna</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wnan315.013926-13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wgmail.com325.027551.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Woutlook.com330.046631.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wyahoo.com476.018132-111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Hgmail.com420.044971.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 619 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_std_isna  isFraud                          id\n",
              "0  1.0  0.0  0.0  ...            1        0         Wnan315.013926-13.0\n",
              "1  1.0  0.0  0.0  ...            1        0      Wgmail.com325.027551.0\n",
              "2  1.0  0.0  0.0  ...            0        0    Woutlook.com330.046631.0\n",
              "3  1.0  0.0  0.0  ...            1        0  Wyahoo.com476.018132-111.0\n",
              "4  0.0  0.0  0.0  ...            1        0      Hgmail.com420.044971.0\n",
              "\n",
              "[5 rows x 619 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f93dd8-09e1-4597-fd4c-b58ef7b04f41"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2793.39 MB\n",
            "Memory usage after optimization is: 678.37 MB\n",
            "Decreased by 75.7%\n",
            "Memory usage of dataframe is 2392.90 MB\n",
            "Memory usage after optimization is: 580.09 MB\n",
            "Decreased by 75.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ"
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW"
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data,epochs):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "        self.epochs=epochs\n",
        "        self.val=0\n",
        "        self.wts=[]\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        if roc_val>self.val:\n",
        "          self.val=roc_val\n",
        "          self.epoch=10\n",
        "          self.wts=self.model.get_weights()\n",
        "        else:\n",
        "          self.epoch-=1\n",
        "        if self.epoch==0:\n",
        "          self.model.set_weights(self.wts)\n",
        "          self.model.stop_training = True\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model(dim):\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "  uid=Input((1,))\n",
        "  inp=Input((873,))\n",
        "  emb=Embedding(input_dim=dim,output_dim=4)(uid)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Concatenate()([emb,x])\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp,uid],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZH6xEokzPfj"
      },
      "source": [
        "Adding all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZw0LXIzPG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e9337f-a2d1-496f-8992-5e7b668f2312"
      },
      "source": [
        "trn_s=trn.shape[0]\n",
        "df=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "autoenc=pd.read_csv('/content/gdrive/My Drive/fraud/without_id.csv',index_col=[0])\n",
        "autoenc=reduce_mem_usage(autoenc)\n",
        "\n",
        "autoenc.columns=[i for i in range(444,444+autoenc.shape[1])]\n",
        "\n",
        "\n",
        "df=pd.concat([df,autoenc],1)\n",
        "del([autoenc])\n",
        "gc.collect()\n",
        "\n",
        "trn=df.loc[:trn_s-1]\n",
        "tst=df.loc[trn_s:].reset_index(drop=True)\n",
        "del([df])\n",
        "gc.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2151.40 MB\n",
            "Memory usage after optimization is: 544.13 MB\n",
            "Decreased by 74.7%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxmq8JSOz6Hk"
      },
      "source": [
        "categorical=[str(i) for i in range(444)]\n",
        "trn[categorical]=trn[categorical].astype('uint8')\n",
        "tst[categorical]=tst[categorical].astype('uint8')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7h25ziHJlo3"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=2\n",
        "        alpha=0.9\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZ7gFNd4HwB"
      },
      "source": [
        "class stocasticensembling(Callback):\r\n",
        "  def __init__(self,model_name,alpha1,alpha2,iter_per_epoch,cycle_len,seqs_dict,start_inx=0,save_se_weights=False,folder='/content',**kwargs):\r\n",
        "    #save_se_weights: save after each epoch ?\r\n",
        "\r\n",
        "    super(stocasticensembling,self).__init__()\r\n",
        "    self.model_count=0\r\n",
        "    self.alpha1=alpha1\r\n",
        "    self.alpha2=alpha2\r\n",
        "    self.clr_iterations=0\r\n",
        "    self.cycle_num=cycle_len\r\n",
        "    self.cycle_len=cycle_len\r\n",
        "    self.iter_per_epoch=iter_per_epoch\r\n",
        "    self.iter_per_cycle = self.cycle_len * self.iter_per_epoch\r\n",
        "    self.save_se_weights=save_se_weights\r\n",
        "    self.start_inx=start_inx\r\n",
        "    self.swa_weights=[]\r\n",
        "    self.folder=folder\r\n",
        "    self.seqs_dict=seqs_dict\r\n",
        "    self.model_name=model_name\r\n",
        "    self.prob_dict={k: [] for k in self.seqs_dict.keys()}\r\n",
        "    self.lrs=[]\r\n",
        "\r\n",
        "  def on_train_end(self,logs={}):\r\n",
        "    self.weight_update()\r\n",
        "    self.model.set_weights(self.swa_weights)\r\n",
        "    self.snapsort()\r\n",
        "    for seq_names,probs in self.prob_dict.items():\r\n",
        "      self.prob_dict[seq_names]=np.concatenate(probs,axis=-1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_epoch_begin(self,epoch,logs=None):\r\n",
        "    self.current_epoch=epoch\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_epoch_end(self,epoch,logs=None):\r\n",
        "    self.cycle_num+=1\r\n",
        "    if (self._t_cycle() !=1) or (epoch == 14):\r\n",
        "      return\r\n",
        "    self.snapsort()\r\n",
        "    self.weight_update()\r\n",
        "    self.model_count+=1\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_batch_begin(self,batch,logs=None):\r\n",
        "    self.clr_iterations+=1\r\n",
        "    lr=self._clr_schedule()\r\n",
        "    self.lrs.append(lr)\r\n",
        "    K.set_value(self.model.optimizer.lr,lr)\r\n",
        "  \r\n",
        "  \r\n",
        "  def snapsort(self):\r\n",
        "    print(self.clr_iterations)\r\n",
        "    print(K.eval(self.model.optimizer.lr))\r\n",
        "    for seq_name,seq in self.seqs_dict.items():\r\n",
        "      self.prob_dict[seq_name].append(self.model.predict(seq,steps=len(seq)))\r\n",
        "  \r\n",
        "  \r\n",
        "  def weight_update(self):\r\n",
        "    weights=self.model.get_weights()\r\n",
        "    if self.model_count==0:\r\n",
        "      self.swa_weights=weights\r\n",
        "    for i in range(0,len(weights)):\r\n",
        "      self.swa_weights[i]=(self.swa_weights[i]*self.model_count+weights[i])/(self.model_count+1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def _t_cycle(self):\r\n",
        "        return (((self.clr_iterations - 1) % self.iter_per_cycle) + 1) / self.iter_per_cycle\r\n",
        "  \r\n",
        "  \r\n",
        "  def _clr_schedule(self):\r\n",
        "    return ((1.0 - 1.0 *self._t_cycle()) * self.alpha2) + (1.0 *self._t_cycle() *self.alpha1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412c7fd6-c9d5-40f6-c2d8-a794798f4b1d"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from matplotlib import pyplot as plt\n",
        "splits=KFold(n_splits=5)\n",
        "gc.collect()\n",
        "pre=np.zeros((506691,1))\n",
        "# tst=tst.drop(['isFraud'],1)\n",
        "for en,(train_index,test_index) in enumerate(tqdm(splits.split(trn))):\n",
        "  X_train, X_test = trn.loc[train_index], trn.loc[test_index]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  ids={}\n",
        "  for en,id in enumerate(X_train['id'].unique()):\n",
        "    ids[id]=en+2\n",
        "  X_train['id']=X_train['id'].map(lambda x: ids.get(x,1))\n",
        "  X_test['id']=X_test['id'].map(lambda x: ids.get(x,1))\n",
        "  dim=X_train['id'].nunique()+2\n",
        "  gc.collect()\n",
        "  trn_id,tst_id=X_train['id'],X_test['id']\n",
        "  X_train=X_train.drop(['isFraud','id'],1)\n",
        "  X_test=X_test.drop(['isFraud','id'],1)\n",
        "  mod=load_model(dim)\n",
        "  roc = RocCallback(validation_data=([X_test,tst_id], y_test),epochs=10)\n",
        "  mod.compile(optimizer=Nadam(),loss=fl())\n",
        "  es=EarlyStopping(monitor='acu_val',min_delta=0.0001,mode='min',restore_best_weights=True,patience=10)\n",
        "  seqs_dict={'test':[tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))]}\n",
        "  se = stocasticensembling(seqs_dict=seqs_dict, cycle_len=5, iter_per_epoch=231,\n",
        "                                  alpha1=5e-4, alpha2=5e-3,\n",
        "                                   model_name=\"model\", verbose=1)\n",
        "  mod.fit([X_train,trn_id],y_train,validation_data=([X_test,tst_id],y_test),batch_size=2048,epochs=15,callbacks=[se])\n",
        "  \n",
        "  del[(X_train,y_train)]\n",
        "  gc.collect()\n",
        "\n",
        "  mod.fit([X_test,tst_id],y_test,epochs=2,batch_size=2048)\n",
        "  pre+=se.prob_dict['test'].mean(1).reshape(506691,1)\n",
        "  pre+=mod.predict([tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))])\n",
        "\n",
        "  del([X_test,y_test,mod])\n",
        "  gc.collect()\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "231/231 [==============================] - 6s 20ms/step - loss: 45.0206 - val_loss: 17.6549\n",
            "Epoch 2/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 10.2691 - val_loss: 17.1068\n",
            "Epoch 3/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 4.7455 - val_loss: 16.9607\n",
            "Epoch 4/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 2.9135 - val_loss: 17.0471\n",
            "Epoch 5/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.1372 - val_loss: 16.6083\n",
            "1155\n",
            "0.0005\n",
            "Epoch 6/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.0679 - val_loss: 19.0198\n",
            "Epoch 7/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.5659 - val_loss: 18.2294\n",
            "Epoch 8/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.3051 - val_loss: 17.6484\n",
            "Epoch 9/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1270 - val_loss: 18.9804\n",
            "Epoch 10/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.9978 - val_loss: 17.8421\n",
            "2310\n",
            "0.0005\n",
            "Epoch 11/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1505 - val_loss: 21.5364\n",
            "Epoch 12/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.2336 - val_loss: 21.1210\n",
            "Epoch 13/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.0547 - val_loss: 20.0111\n",
            "Epoch 14/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.9500 - val_loss: 19.8666\n",
            "Epoch 15/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.9217 - val_loss: 19.2829\n",
            "3465\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 14.9402\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 12.8724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1it [01:54, 114.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "231/231 [==============================] - 6s 19ms/step - loss: 41.4313 - val_loss: 21.2668\n",
            "Epoch 2/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 10.4115 - val_loss: 17.8315\n",
            "Epoch 3/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 4.6094 - val_loss: 17.6536\n",
            "Epoch 4/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.7490 - val_loss: 17.4880\n",
            "Epoch 5/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.0895 - val_loss: 17.5304\n",
            "1155\n",
            "0.0005\n",
            "Epoch 6/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.9639 - val_loss: 18.8141\n",
            "Epoch 7/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.4730 - val_loss: 18.5720\n",
            "Epoch 8/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.2237 - val_loss: 19.0234\n",
            "Epoch 9/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.0720 - val_loss: 18.9040\n",
            "Epoch 10/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.9567 - val_loss: 18.9568\n",
            "2310\n",
            "0.0005\n",
            "Epoch 11/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1682 - val_loss: 21.5163\n",
            "Epoch 12/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1086 - val_loss: 19.8882\n",
            "Epoch 13/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.0001 - val_loss: 21.5586\n",
            "Epoch 14/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.9199 - val_loss: 20.7267\n",
            "Epoch 15/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.8447 - val_loss: 20.6152\n",
            "3465\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 16.5771\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 14.2370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r2it [03:43, 112.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "231/231 [==============================] - 6s 19ms/step - loss: 44.8034 - val_loss: 20.5334\n",
            "Epoch 2/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 10.5997 - val_loss: 17.2658\n",
            "Epoch 3/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 4.7159 - val_loss: 17.3860\n",
            "Epoch 4/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.8263 - val_loss: 17.4083\n",
            "Epoch 5/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.1818 - val_loss: 17.3317\n",
            "1155\n",
            "0.0005\n",
            "Epoch 6/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.0397 - val_loss: 18.7144\n",
            "Epoch 7/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.4289 - val_loss: 19.0847\n",
            "Epoch 8/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1676 - val_loss: 19.1685\n",
            "Epoch 9/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1213 - val_loss: 19.5064\n",
            "Epoch 10/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.9883 - val_loss: 19.6555\n",
            "2310\n",
            "0.0005\n",
            "Epoch 11/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1357 - val_loss: 20.3961\n",
            "Epoch 12/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.1189 - val_loss: 20.4333\n",
            "Epoch 13/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 1.0275 - val_loss: 20.4031\n",
            "Epoch 14/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.9857 - val_loss: 20.9982\n",
            "Epoch 15/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.8296 - val_loss: 20.8892\n",
            "3465\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 16.6928\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 14.2817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r3it [05:32, 111.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "231/231 [==============================] - 6s 19ms/step - loss: 41.9582 - val_loss: 18.2461\n",
            "Epoch 2/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 10.4482 - val_loss: 16.2034\n",
            "Epoch 3/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 4.6182 - val_loss: 15.9758\n",
            "Epoch 4/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 2.7948 - val_loss: 15.6065\n",
            "Epoch 5/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 2.1005 - val_loss: 15.8880\n",
            "1155\n",
            "0.0005\n",
            "Epoch 6/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.0054 - val_loss: 16.8610\n",
            "Epoch 7/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.4511 - val_loss: 17.2600\n",
            "Epoch 8/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.2019 - val_loss: 16.7813\n",
            "Epoch 9/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.0672 - val_loss: 17.4832\n",
            "Epoch 10/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.9481 - val_loss: 17.2771\n",
            "2310\n",
            "0.0005\n",
            "Epoch 11/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.1486 - val_loss: 18.0605\n",
            "Epoch 12/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.1460 - val_loss: 19.1548\n",
            "Epoch 13/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.9896 - val_loss: 18.6700\n",
            "Epoch 14/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.8672 - val_loss: 19.4248\n",
            "Epoch 15/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.8049 - val_loss: 19.0218\n",
            "3465\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 15.4186\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 13.4143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r4it [07:15, 109.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "231/231 [==============================] - 6s 19ms/step - loss: 43.7569 - val_loss: 20.0293\n",
            "Epoch 2/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 10.2772 - val_loss: 17.5067\n",
            "Epoch 3/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 4.5520 - val_loss: 17.7806\n",
            "Epoch 4/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 2.7705 - val_loss: 17.6657\n",
            "Epoch 5/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 2.1376 - val_loss: 17.5605\n",
            "1155\n",
            "0.0005\n",
            "Epoch 6/15\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 2.0094 - val_loss: 18.2498\n",
            "Epoch 7/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.5010 - val_loss: 19.2791\n",
            "Epoch 8/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.2582 - val_loss: 18.7045\n",
            "Epoch 9/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.1881 - val_loss: 18.5896\n",
            "Epoch 10/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.0072 - val_loss: 19.0108\n",
            "2310\n",
            "0.0005\n",
            "Epoch 11/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.2286 - val_loss: 18.4477\n",
            "Epoch 12/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.2035 - val_loss: 18.6769\n",
            "Epoch 13/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 1.0722 - val_loss: 18.6739\n",
            "Epoch 14/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.9718 - val_loss: 19.2672\n",
            "Epoch 15/15\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.8775 - val_loss: 19.4219\n",
            "3465\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 16.4350\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 14.4841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5it [08:56, 107.31s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "82226e9b-132d-48c5-bd7f-b63a1c3f057a"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre/10\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.023884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.044912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.121875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.110348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.218639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.023884\n",
              "3663550        0.044912\n",
              "3663551        0.121875\n",
              "3663552        0.110348\n",
              "3663553        0.218639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo"
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "233239bf-db97-4eee-9063-1d58df1bbe85"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre/10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efd4cbb7048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeWMkwJIWng"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}

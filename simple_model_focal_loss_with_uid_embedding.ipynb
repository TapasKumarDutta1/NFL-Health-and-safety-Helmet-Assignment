{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_with_uid_embedding",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_with_uid_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "3b758b86-b436-440d-9c4d-db5b691ff520"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv",
        "colab_type": "text"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d75b7ce9-12aa-43fb-84dc-a45d4a7f49cc"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af",
        "colab_type": "text"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "33932902-02e3-4be1-dfa6-6376cb1be64e"
      },
      "source": [
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train_id.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test_id.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C9_std_isna</th>\n",
              "      <th>C10_std_isna</th>\n",
              "      <th>C11_std_isna</th>\n",
              "      <th>C12_std_isna</th>\n",
              "      <th>C13_std_isna</th>\n",
              "      <th>C14_std_isna</th>\n",
              "      <th>D1_mean_isna</th>\n",
              "      <th>D1_std_isna</th>\n",
              "      <th>D2_mean_isna</th>\n",
              "      <th>D2_std_isna</th>\n",
              "      <th>D3_mean_isna</th>\n",
              "      <th>D3_std_isna</th>\n",
              "      <th>D4_mean_isna</th>\n",
              "      <th>D4_std_isna</th>\n",
              "      <th>D5_mean_isna</th>\n",
              "      <th>D5_std_isna</th>\n",
              "      <th>D6_mean_isna</th>\n",
              "      <th>D6_std_isna</th>\n",
              "      <th>D7_mean_isna</th>\n",
              "      <th>D7_std_isna</th>\n",
              "      <th>D8_mean_isna</th>\n",
              "      <th>D8_std_isna</th>\n",
              "      <th>D9_mean_isna</th>\n",
              "      <th>D9_std_isna</th>\n",
              "      <th>D10_mean_isna</th>\n",
              "      <th>D10_std_isna</th>\n",
              "      <th>D11_mean_isna</th>\n",
              "      <th>D11_std_isna</th>\n",
              "      <th>D12_mean_isna</th>\n",
              "      <th>D12_std_isna</th>\n",
              "      <th>D13_mean_isna</th>\n",
              "      <th>D13_std_isna</th>\n",
              "      <th>D14_mean_isna</th>\n",
              "      <th>D14_std_isna</th>\n",
              "      <th>D15_mean_isna</th>\n",
              "      <th>D15_std_isna</th>\n",
              "      <th>V1_mean_isna</th>\n",
              "      <th>V1_std_isna</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wnan315.013926-13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wgmail.com325.027551.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Woutlook.com330.046631.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wyahoo.com476.018132-111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Hgmail.com420.044971.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 619 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_std_isna  isFraud                          id\n",
              "0  1.0  0.0  0.0  ...            1        0         Wnan315.013926-13.0\n",
              "1  1.0  0.0  0.0  ...            1        0      Wgmail.com325.027551.0\n",
              "2  1.0  0.0  0.0  ...            0        0    Woutlook.com330.046631.0\n",
              "3  1.0  0.0  0.0  ...            1        0  Wyahoo.com476.018132-111.0\n",
              "4  0.0  0.0  0.0  ...            1        0      Hgmail.com420.044971.0\n",
              "\n",
              "[5 rows x 619 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC",
        "colab_type": "text"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "2d739073-aadb-4e8e-c917-a5bfdf895728"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2793.39 MB\n",
            "Memory usage after optimization is: 678.37 MB\n",
            "Decreased by 75.7%\n",
            "Memory usage of dataframe is 2392.90 MB\n",
            "Memory usage after optimization is: 580.09 MB\n",
            "Decreased by 75.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6",
        "colab_type": "text"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data,epochs):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "        self.epochs=epochs\n",
        "        self.val=0\n",
        "        self.wts=[]\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        if roc_val>self.val:\n",
        "          self.val=roc_val\n",
        "          self.epoch=10\n",
        "          self.wts=self.model.get_weights()\n",
        "        else:\n",
        "          self.epoch-=1\n",
        "        if self.epoch==0:\n",
        "          self.model.set_weights(self.wts)\n",
        "          self.model.stop_training = True\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model(dim):\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "  uid=Input((1,))\n",
        "  inp=Input((873,))\n",
        "  emb=Embedding(input_dim=dim,output_dim=4)(uid)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Concatenate()([emb,x])\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp,uid],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZH6xEokzPfj",
        "colab_type": "text"
      },
      "source": [
        "Adding all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZw0LXIzPG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "99e90924-4443-4dd1-e83e-0e7ee4ffd119"
      },
      "source": [
        "trn_s=trn.shape[0]\n",
        "df=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "autoenc=pd.read_csv('/content/gdrive/My Drive/fraud/without_id.csv',index_col=[0])\n",
        "autoenc=reduce_mem_usage(autoenc)\n",
        "\n",
        "autoenc.columns=[i for i in range(444,444+autoenc.shape[1])]\n",
        "\n",
        "\n",
        "df=pd.concat([df,autoenc],1)\n",
        "del([autoenc])\n",
        "gc.collect()\n",
        "\n",
        "trn=df.loc[:trn_s-1]\n",
        "tst=df.loc[trn_s:].reset_index(drop=True)\n",
        "del([df])\n",
        "gc.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2151.40 MB\n",
            "Memory usage after optimization is: 544.13 MB\n",
            "Decreased by 74.7%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxmq8JSOz6Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical=[str(i) for i in range(444)]\n",
        "trn[categorical]=trn[categorical].astype('uint8')\n",
        "tst[categorical]=tst[categorical].astype('uint8')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7h25ziHJlo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=4\n",
        "        alpha=0.25\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64fa88cb-421e-4630-a19e-4b5630423bae"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "splits=KFold(n_splits=5)\n",
        "gc.collect()\n",
        "pre=np.zeros((506691,1))\n",
        "tst=tst.drop(['isFraud'],1)\n",
        "for train_index,test_index in tqdm(splits.split(trn)):\n",
        "  X_train, X_test = trn.loc[train_index], trn.loc[test_index]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  ids={}\n",
        "  for en,id in enumerate(X_train['id'].unique()):\n",
        "    ids[id]=en+2\n",
        "  X_train['id']=X_train['id'].map(lambda x: ids.get(x,1))\n",
        "  X_test['id']=X_test['id'].map(lambda x: ids.get(x,1))\n",
        "  dim=X_train['id'].nunique()+2\n",
        "  gc.collect()\n",
        "  trn_id,tst_id=X_train['id'],X_test['id']\n",
        "  X_train=X_train.drop(['isFraud','id'],1)\n",
        "  X_test=X_test.drop(['isFraud','id'],1)\n",
        "  mod=load_model(dim)\n",
        "  roc = RocCallback(validation_data=([X_test,tst_id], y_test),epochs=10)\n",
        "  mod.compile(optimizer=Nadam(),loss=fl())\n",
        "  es=EarlyStopping(monitor='acu_val',min_delta=0.0001,mode='min',restore_best_weights=True,patience=10)\n",
        "  mod.fit([X_train,trn_id],y_train,validation_data=([X_test,tst_id],y_test),batch_size=2048,epochs=50,callbacks=[roc])\n",
        "  \n",
        "  del[(X_train,y_train)]\n",
        "  gc.collect()\n",
        "\n",
        "  mod.fit([X_test,tst_id],y_test,epochs=2,batch_size=2048)\n",
        "  pre+=mod.predict([tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))])/5\n",
        "  \n",
        "  del([X_test,y_test,mod])\n",
        "  gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 15.4080roc-auc_val: 0.8616\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 15.3213 - val_loss: 6.5784\n",
            "Epoch 2/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 7.3416roc-auc_val: 0.9005\n",
            "231/231 [==============================] - 8s 34ms/step - loss: 7.3272 - val_loss: 5.0593\n",
            "Epoch 3/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 5.2593roc-auc_val: 0.9124\n",
            "231/231 [==============================] - 8s 34ms/step - loss: 5.2593 - val_loss: 4.4871\n",
            "Epoch 4/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 3.7958roc-auc_val: 0.9131\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 3.7902 - val_loss: 5.4371\n",
            "Epoch 5/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 2.8310roc-auc_val: 0.922\n",
            "231/231 [==============================] - 9s 37ms/step - loss: 2.8222 - val_loss: 5.0434\n",
            "Epoch 6/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 2.1537roc-auc_val: 0.9257\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 2.1511 - val_loss: 5.0791\n",
            "Epoch 7/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 1.6836roc-auc_val: 0.9264\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.6815 - val_loss: 5.1522\n",
            "Epoch 8/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 1.3532roc-auc_val: 0.9287\n",
            "231/231 [==============================] - 8s 34ms/step - loss: 1.3525 - val_loss: 4.3082\n",
            "Epoch 9/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 1.1444roc-auc_val: 0.9237\n",
            "231/231 [==============================] - 8s 35ms/step - loss: 1.1444 - val_loss: 5.0232\n",
            "Epoch 10/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.9467roc-auc_val: 0.9249\n",
            "231/231 [==============================] - 8s 35ms/step - loss: 0.9469 - val_loss: 5.3322\n",
            "Epoch 11/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.8268roc-auc_val: 0.9276\n",
            "231/231 [==============================] - 7s 31ms/step - loss: 0.8245 - val_loss: 4.7179\n",
            "Epoch 12/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.7558roc-auc_val: 0.9204\n",
            "231/231 [==============================] - 8s 35ms/step - loss: 0.7559 - val_loss: 5.0349\n",
            "Epoch 13/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.6608roc-auc_val: 0.9241\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.6588 - val_loss: 5.2085\n",
            "Epoch 14/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 0.5999roc-auc_val: 0.9281\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.6006 - val_loss: 4.5916\n",
            "Epoch 15/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.5403roc-auc_val: 0.9257\n",
            "231/231 [==============================] - 8s 35ms/step - loss: 0.5400 - val_loss: 4.6253\n",
            "Epoch 16/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.5234roc-auc_val: 0.9186\n",
            "231/231 [==============================] - 8s 35ms/step - loss: 0.5234 - val_loss: 4.9758\n",
            "Epoch 17/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.4803roc-auc_val: 0.9248\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 0.4798 - val_loss: 4.7252\n",
            "Epoch 18/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.4601roc-auc_val: 0.9234\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.4598 - val_loss: 4.8385\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 3.9420\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 3.3732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1it [02:55, 175.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 16.9808roc-auc_val: 0.88\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 16.8766 - val_loss: 7.1303\n",
            "Epoch 2/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 7.5703roc-auc_val: 0.9118\n",
            "231/231 [==============================] - 8s 32ms/step - loss: 7.5589 - val_loss: 6.2142\n",
            "Epoch 3/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 5.3672roc-auc_val: 0.9248\n",
            "231/231 [==============================] - 8s 34ms/step - loss: 5.3625 - val_loss: 5.6579\n",
            "Epoch 4/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 3.9053roc-auc_val: 0.9407\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 3.8987 - val_loss: 5.0646\n",
            "Epoch 5/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 2.8876roc-auc_val: 0.9378\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 2.8812 - val_loss: 5.3254\n",
            "Epoch 6/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 2.2593roc-auc_val: 0.9487\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 2.2557 - val_loss: 4.7596\n",
            "Epoch 7/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.7131roc-auc_val: 0.9474\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.7176 - val_loss: 4.8179\n",
            "Epoch 8/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 1.3749roc-auc_val: 0.9518\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 1.3731 - val_loss: 4.7609\n",
            "Epoch 9/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.1475roc-auc_val: 0.9508\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.1436 - val_loss: 4.7585\n",
            "Epoch 10/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.9617roc-auc_val: 0.9515\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.9618 - val_loss: 4.7305\n",
            "Epoch 11/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.8325roc-auc_val: 0.9499\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.8318 - val_loss: 4.9616\n",
            "Epoch 12/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.7448roc-auc_val: 0.9479\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.7436 - val_loss: 4.9750\n",
            "Epoch 13/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.6568roc-auc_val: 0.95\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.6568 - val_loss: 4.8634\n",
            "Epoch 14/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.6161roc-auc_val: 0.9476\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6152 - val_loss: 4.9896\n",
            "Epoch 15/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.5480roc-auc_val: 0.9485\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.5473 - val_loss: 4.9520\n",
            "Epoch 16/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 0.5321roc-auc_val: 0.9486\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.5313 - val_loss: 5.0370\n",
            "Epoch 17/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.4805roc-auc_val: 0.945\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.4870 - val_loss: 5.1612\n",
            "Epoch 18/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.4647roc-auc_val: 0.9478\n",
            "231/231 [==============================] - 8s 32ms/step - loss: 0.4645 - val_loss: 5.1267\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 4.8164\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 4.1031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r2it [05:38, 171.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 12.8514roc-auc_val: 0.8841\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 12.8252 - val_loss: 6.8052\n",
            "Epoch 2/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 6.3960roc-auc_val: 0.9121\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 6.3941 - val_loss: 5.7126\n",
            "Epoch 3/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 4.6478roc-auc_val: 0.9292\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 4.6348 - val_loss: 5.1561\n",
            "Epoch 4/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 3.4106roc-auc_val: 0.9378\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 3.4066 - val_loss: 4.9720\n",
            "Epoch 5/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 2.5581roc-auc_val: 0.9468\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 2.5581 - val_loss: 4.5613\n",
            "Epoch 6/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.9735roc-auc_val: 0.9503\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 1.9701 - val_loss: 4.4907\n",
            "Epoch 7/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 1.5595roc-auc_val: 0.9495\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.5570 - val_loss: 4.4134\n",
            "Epoch 8/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 1.2585roc-auc_val: 0.9504\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.2585 - val_loss: 4.4208\n",
            "Epoch 9/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 1.0466roc-auc_val: 0.9487\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.0453 - val_loss: 4.5157\n",
            "Epoch 10/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.8944roc-auc_val: 0.9484\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.8941 - val_loss: 4.5016\n",
            "Epoch 11/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.7819roc-auc_val: 0.9485\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.7808 - val_loss: 4.5061\n",
            "Epoch 12/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.6832roc-auc_val: 0.9493\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.6832 - val_loss: 4.5404\n",
            "Epoch 13/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.6255roc-auc_val: 0.948\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.6232 - val_loss: 4.5634\n",
            "Epoch 14/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.5727roc-auc_val: 0.9462\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.5720 - val_loss: 4.6609\n",
            "Epoch 15/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 0.5254roc-auc_val: 0.9444\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.5242 - val_loss: 4.7872\n",
            "Epoch 16/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.4950roc-auc_val: 0.9462\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.4950 - val_loss: 4.7767\n",
            "Epoch 17/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.4689roc-auc_val: 0.9439\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.4688 - val_loss: 4.9191\n",
            "Epoch 18/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.4617roc-auc_val: 0.9447\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.4610 - val_loss: 4.9728\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 4.4040\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 3.7524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r3it [08:21, 169.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 22.4118roc-auc_val: 0.8946\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 22.3299 - val_loss: 6.6282\n",
            "Epoch 2/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 8.6481roc-auc_val: 0.9144\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 8.6239 - val_loss: 6.4035\n",
            "Epoch 3/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 5.9027roc-auc_val: 0.9357\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 5.8970 - val_loss: 5.5448\n",
            "Epoch 4/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 4.2755roc-auc_val: 0.9449\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 4.2710 - val_loss: 5.0905\n",
            "Epoch 5/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 3.1876roc-auc_val: 0.9525\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 3.1852 - val_loss: 4.6955\n",
            "Epoch 6/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 2.4223roc-auc_val: 0.9578\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 2.4223 - val_loss: 4.4409\n",
            "Epoch 7/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.8602roc-auc_val: 0.9596\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.8581 - val_loss: 4.3209\n",
            "Epoch 8/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.4989roc-auc_val: 0.9623\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.4950 - val_loss: 4.2388\n",
            "Epoch 9/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.2208roc-auc_val: 0.9626\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.2190 - val_loss: 4.1941\n",
            "Epoch 10/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.0433roc-auc_val: 0.9611\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.0418 - val_loss: 4.2294\n",
            "Epoch 11/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.8814roc-auc_val: 0.9619\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.8814 - val_loss: 4.2286\n",
            "Epoch 12/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.7674roc-auc_val: 0.9607\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.7674 - val_loss: 4.2441\n",
            "Epoch 13/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 0.6740roc-auc_val: 0.9615\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6734 - val_loss: 4.2294\n",
            "Epoch 14/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.5935roc-auc_val: 0.9609\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.5945 - val_loss: 4.2985\n",
            "Epoch 15/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.5791roc-auc_val: 0.9597\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.5784 - val_loss: 4.3206\n",
            "Epoch 16/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.5297roc-auc_val: 0.9601\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.5297 - val_loss: 4.4032\n",
            "Epoch 17/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.5086roc-auc_val: 0.9584\n",
            "231/231 [==============================] - 8s 32ms/step - loss: 0.5080 - val_loss: 4.4699\n",
            "Epoch 18/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.4903roc-auc_val: 0.9582\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.4899 - val_loss: 4.5233\n",
            "Epoch 19/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.4538roc-auc_val: 0.9586\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 0.4524 - val_loss: 4.5470\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 4.4985\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 3.9431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r4it [11:14, 170.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 17.6380roc-auc_val: 0.8691\n",
            "231/231 [==============================] - 8s 36ms/step - loss: 17.6049 - val_loss: 6.9864\n",
            "Epoch 2/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 7.6128roc-auc_val: 0.8927\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 7.6089 - val_loss: 6.4239\n",
            "Epoch 3/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 5.4514roc-auc_val: 0.9099\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 5.4267 - val_loss: 5.8968\n",
            "Epoch 4/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 3.9852roc-auc_val: 0.9239\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 3.9760 - val_loss: 5.2723\n",
            "Epoch 5/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 2.9227roc-auc_val: 0.9316\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 2.9166 - val_loss: 5.1107\n",
            "Epoch 6/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 2.2150roc-auc_val: 0.934\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 2.2150 - val_loss: 4.9313\n",
            "Epoch 7/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 1.7539roc-auc_val: 0.9385\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.7511 - val_loss: 4.8232\n",
            "Epoch 8/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 1.3746roc-auc_val: 0.9408\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.3730 - val_loss: 4.6914\n",
            "Epoch 9/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1.1434roc-auc_val: 0.9426\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 1.1425 - val_loss: 4.6466\n",
            "Epoch 10/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.9721roc-auc_val: 0.9411\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.9701 - val_loss: 4.8095\n",
            "Epoch 11/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.8260roc-auc_val: 0.942\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.8240 - val_loss: 4.6994\n",
            "Epoch 12/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.7291roc-auc_val: 0.9388\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.7278 - val_loss: 4.7577\n",
            "Epoch 13/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 0.6413roc-auc_val: 0.9399\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6413 - val_loss: 4.8221\n",
            "Epoch 14/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.5926roc-auc_val: 0.9398\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.5915 - val_loss: 4.8106\n",
            "Epoch 15/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.5441roc-auc_val: 0.9422\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.5443 - val_loss: 4.7347\n",
            "Epoch 16/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 0.5021roc-auc_val: 0.9403\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 0.5021 - val_loss: 4.7361\n",
            "Epoch 17/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.4844roc-auc_val: 0.9404\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.4843 - val_loss: 4.8223\n",
            "Epoch 18/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 0.4694roc-auc_val: 0.9377\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.4677 - val_loss: 4.9785\n",
            "Epoch 19/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 0.4333roc-auc_val: 0.9368\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.4340 - val_loss: 4.9523\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 4.7803\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 4.1959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5it [14:05, 169.09s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "d3e04473-be18-441f-f11b-0c5abbc61228"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.056458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.070049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.152741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.141143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.079093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.056458\n",
              "3663550        0.070049\n",
              "3663551        0.152741\n",
              "3663552        0.141143\n",
              "3663553        0.079093"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "cae32eae-e4e7-4ff9-9490-df5e7e25965b"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2a6264358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeWMkwJIWng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_with_uid_embedding",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_with_uid_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26efdc0-a7f6-415a-fdf7-0b1a3238b66b"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1611181d-7fe4-4582-ec73-0127ebfdaf54"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "15414098-6301-4929-9e1b-68fec0a2dd29"
      },
      "source": [
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train_id.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test_id.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C9_std_isna</th>\n",
              "      <th>C10_std_isna</th>\n",
              "      <th>C11_std_isna</th>\n",
              "      <th>C12_std_isna</th>\n",
              "      <th>C13_std_isna</th>\n",
              "      <th>C14_std_isna</th>\n",
              "      <th>D1_mean_isna</th>\n",
              "      <th>D1_std_isna</th>\n",
              "      <th>D2_mean_isna</th>\n",
              "      <th>D2_std_isna</th>\n",
              "      <th>D3_mean_isna</th>\n",
              "      <th>D3_std_isna</th>\n",
              "      <th>D4_mean_isna</th>\n",
              "      <th>D4_std_isna</th>\n",
              "      <th>D5_mean_isna</th>\n",
              "      <th>D5_std_isna</th>\n",
              "      <th>D6_mean_isna</th>\n",
              "      <th>D6_std_isna</th>\n",
              "      <th>D7_mean_isna</th>\n",
              "      <th>D7_std_isna</th>\n",
              "      <th>D8_mean_isna</th>\n",
              "      <th>D8_std_isna</th>\n",
              "      <th>D9_mean_isna</th>\n",
              "      <th>D9_std_isna</th>\n",
              "      <th>D10_mean_isna</th>\n",
              "      <th>D10_std_isna</th>\n",
              "      <th>D11_mean_isna</th>\n",
              "      <th>D11_std_isna</th>\n",
              "      <th>D12_mean_isna</th>\n",
              "      <th>D12_std_isna</th>\n",
              "      <th>D13_mean_isna</th>\n",
              "      <th>D13_std_isna</th>\n",
              "      <th>D14_mean_isna</th>\n",
              "      <th>D14_std_isna</th>\n",
              "      <th>D15_mean_isna</th>\n",
              "      <th>D15_std_isna</th>\n",
              "      <th>V1_mean_isna</th>\n",
              "      <th>V1_std_isna</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wnan315.013926-13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wgmail.com325.027551.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Woutlook.com330.046631.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wyahoo.com476.018132-111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Hgmail.com420.044971.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 619 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_std_isna  isFraud                          id\n",
              "0  1.0  0.0  0.0  ...            1        0         Wnan315.013926-13.0\n",
              "1  1.0  0.0  0.0  ...            1        0      Wgmail.com325.027551.0\n",
              "2  1.0  0.0  0.0  ...            0        0    Woutlook.com330.046631.0\n",
              "3  1.0  0.0  0.0  ...            1        0  Wyahoo.com476.018132-111.0\n",
              "4  0.0  0.0  0.0  ...            1        0      Hgmail.com420.044971.0\n",
              "\n",
              "[5 rows x 619 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502ffa80-a71c-463d-cd7d-8c84225392a6"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2793.39 MB\n",
            "Memory usage after optimization is: 678.37 MB\n",
            "Decreased by 75.7%\n",
            "Memory usage of dataframe is 2392.90 MB\n",
            "Memory usage after optimization is: 580.09 MB\n",
            "Decreased by 75.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ"
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW"
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data,epochs):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "        self.epochs=epochs\n",
        "        self.val=0\n",
        "        self.wts=[]\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        if roc_val>self.val:\n",
        "          self.val=roc_val\n",
        "          self.epoch=10\n",
        "          self.wts=self.model.get_weights()\n",
        "        else:\n",
        "          self.epoch-=1\n",
        "        if self.epoch==0:\n",
        "          self.model.set_weights(self.wts)\n",
        "          self.model.stop_training = True\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model(dim):\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "  uid=Input((1,))\n",
        "  inp=Input((873,))\n",
        "  emb=Embedding(input_dim=dim,output_dim=4)(uid)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Concatenate()([emb,x])\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp,uid],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZH6xEokzPfj"
      },
      "source": [
        "Adding all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZw0LXIzPG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c4ddd6-7ed6-4640-ee25-5d5d1ed379d2"
      },
      "source": [
        "trn_s=trn.shape[0]\n",
        "df=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "autoenc=pd.read_csv('/content/gdrive/My Drive/fraud/without_id.csv',index_col=[0])\n",
        "autoenc=reduce_mem_usage(autoenc)\n",
        "\n",
        "autoenc.columns=[i for i in range(444,444+autoenc.shape[1])]\n",
        "\n",
        "\n",
        "df=pd.concat([df,autoenc],1)\n",
        "del([autoenc])\n",
        "gc.collect()\n",
        "\n",
        "trn=df.loc[:trn_s-1]\n",
        "tst=df.loc[trn_s:].reset_index(drop=True)\n",
        "del([df])\n",
        "gc.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2151.40 MB\n",
            "Memory usage after optimization is: 544.13 MB\n",
            "Decreased by 74.7%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxmq8JSOz6Hk"
      },
      "source": [
        "categorical=[str(i) for i in range(444)]\n",
        "trn[categorical]=trn[categorical].astype('uint8')\n",
        "tst[categorical]=tst[categorical].astype('uint8')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVjoANI76nSZ"
      },
      "source": [
        "class stocasticensembling(Callback):\r\n",
        "  def __init__(self,model_name,alpha1,alpha2,iter_per_epoch,cycle_len,seqs_dict,start_inx=0,save_se_weights=False,folder='/content',**kwargs):\r\n",
        "    #save_se_weights: save after each epoch ?\r\n",
        "\r\n",
        "    super(stocasticensembling,self).__init__()\r\n",
        "    self.model_count=0\r\n",
        "    self.alpha1=alpha1\r\n",
        "    self.alpha2=alpha2\r\n",
        "    self.clr_iterations=0\r\n",
        "    self.cycle_num=cycle_len\r\n",
        "    self.cycle_len=cycle_len\r\n",
        "    self.iter_per_epoch=iter_per_epoch\r\n",
        "    self.iter_per_cycle = self.cycle_len * self.iter_per_epoch\r\n",
        "    self.save_se_weights=save_se_weights\r\n",
        "    self.start_inx=start_inx\r\n",
        "    self.swa_weights=[]\r\n",
        "    self.folder=folder\r\n",
        "    self.seqs_dict=seqs_dict\r\n",
        "    self.model_name=model_name\r\n",
        "    self.prob_dict={k: [] for k in self.seqs_dict.keys()}\r\n",
        "    self.lrs=[]\r\n",
        "\r\n",
        "  def on_train_end(self,logs={}):\r\n",
        "    self.weight_update()\r\n",
        "    self.model.set_weights(self.swa_weights)\r\n",
        "    self.snapsort()\r\n",
        "    for seq_names,probs in self.prob_dict.items():\r\n",
        "      self.prob_dict[seq_names]=np.concatenate(probs,axis=-1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_epoch_begin(self,epoch,logs=None):\r\n",
        "    self.current_epoch=epoch\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_epoch_end(self,epoch,logs=None):\r\n",
        "    self.cycle_num+=1\r\n",
        "    if (self._t_cycle() !=1) or (epoch == 17):\r\n",
        "      return\r\n",
        "    self.snapsort()\r\n",
        "    self.weight_update()\r\n",
        "    self.model_count+=1\r\n",
        "  \r\n",
        "  \r\n",
        "  def on_batch_begin(self,batch,logs=None):\r\n",
        "    self.clr_iterations+=1\r\n",
        "    lr=self._clr_schedule()\r\n",
        "    self.lrs.append(lr)\r\n",
        "    K.set_value(self.model.optimizer.lr,lr)\r\n",
        "  \r\n",
        "  \r\n",
        "  def snapsort(self):\r\n",
        "    print(self.clr_iterations)\r\n",
        "    print(K.eval(self.model.optimizer.lr))\r\n",
        "    for seq_name,seq in self.seqs_dict.items():\r\n",
        "      self.prob_dict[seq_name].append(self.model.predict(seq,steps=len(seq)))\r\n",
        "  \r\n",
        "  \r\n",
        "  def weight_update(self):\r\n",
        "    weights=self.model.get_weights()\r\n",
        "    if self.model_count==0:\r\n",
        "      self.swa_weights=weights\r\n",
        "    for i in range(0,len(weights)):\r\n",
        "      self.swa_weights[i]=(self.swa_weights[i]*self.model_count+weights[i])/(self.model_count+1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def _t_cycle(self):\r\n",
        "        return (((self.clr_iterations - 1) % self.iter_per_cycle) + 1) / self.iter_per_cycle\r\n",
        "  \r\n",
        "  \r\n",
        "  def _clr_schedule(self):\r\n",
        "    return ((1.0 - 1.0 *self._t_cycle()) * self.alpha2) + (1.0 *self._t_cycle() *self.alpha1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98427e2e-1c8b-4d19-e7da-b7c216228ef9"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "splits=KFold(n_splits=5)\n",
        "gc.collect()\n",
        "pre=np.zeros((506691,1))\n",
        "# tst=tst.drop(['isFraud'],1)\n",
        "for train_index,test_index in tqdm(splits.split(trn)):\n",
        "  X_train, X_test = trn.loc[train_index], trn.loc[test_index]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  ids={}\n",
        "  for en,id in enumerate(X_train['id'].unique()):\n",
        "    ids[id]=en+2\n",
        "  X_train['id']=X_train['id'].map(lambda x: ids.get(x,1))\n",
        "  X_test['id']=X_test['id'].map(lambda x: ids.get(x,1))\n",
        "  dim=X_train['id'].nunique()+2\n",
        "  gc.collect()\n",
        "  trn_id,tst_id=X_train['id'],X_test['id']\n",
        "  X_train=X_train.drop(['isFraud','id'],1)\n",
        "  X_test=X_test.drop(['isFraud','id'],1)\n",
        "  seqs_dict={'test':[tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))]}\n",
        "  se = stocasticensembling(seqs_dict=seqs_dict, cycle_len=6, iter_per_epoch=231,\n",
        "                                  alpha1=5e-4, alpha2=5e-3,\n",
        "                                   model_name=\"model\", verbose=1)\n",
        "  mod=load_model(dim)\n",
        "  roc = RocCallback(validation_data=([X_test,tst_id], y_test),epochs=10)\n",
        "  mod.compile(optimizer=Nadam(),loss='binary_crossentropy')\n",
        "  es=EarlyStopping(monitor='acu_val',min_delta=0.0001,mode='min',restore_best_weights=True,patience=10)\n",
        "  mod.fit([X_train,trn_id],y_train,validation_data=([X_test,tst_id],y_test),batch_size=2048,epochs=18,callbacks=[se])\n",
        "  \n",
        "  del[(X_train,y_train)]\n",
        "  gc.collect()\n",
        "\n",
        "  mod.fit([X_test,tst_id],y_test,epochs=2,batch_size=2048)\n",
        "  pre+=se.prob_dict['test'].mean(1).reshape(506691,1)\n",
        "  pre+=mod.predict([tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))])\n",
        "\n",
        "  \n",
        "  del([X_test,y_test,mod])\n",
        "  gc.collect()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "231/231 [==============================] - 8s 19ms/step - loss: 0.1368 - val_loss: 0.0847\n",
            "Epoch 2/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0619 - val_loss: 0.0872\n",
            "Epoch 3/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0370 - val_loss: 0.0740\n",
            "Epoch 4/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0249 - val_loss: 0.0694\n",
            "Epoch 5/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0183 - val_loss: 0.0687\n",
            "Epoch 6/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0153 - val_loss: 0.0661\n",
            "1386\n",
            "0.0005\n",
            "Epoch 7/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0152 - val_loss: 0.0712\n",
            "Epoch 8/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0107 - val_loss: 0.0673\n",
            "Epoch 9/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0084 - val_loss: 0.0700\n",
            "Epoch 10/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0076 - val_loss: 0.0694\n",
            "Epoch 11/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0065 - val_loss: 0.0741\n",
            "Epoch 12/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0063 - val_loss: 0.0687\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0074 - val_loss: 0.0711\n",
            "Epoch 14/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0072 - val_loss: 0.0733\n",
            "Epoch 15/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0066 - val_loss: 0.0755\n",
            "Epoch 16/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0059 - val_loss: 0.0802\n",
            "Epoch 17/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0051 - val_loss: 0.0816\n",
            "Epoch 18/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0048 - val_loss: 0.0806\n",
            "4158\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.0592\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.0525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1it [02:09, 129.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "231/231 [==============================] - 6s 19ms/step - loss: 0.1393 - val_loss: 0.0910\n",
            "Epoch 2/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0610 - val_loss: 0.0809\n",
            "Epoch 3/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0364 - val_loss: 0.0777\n",
            "Epoch 4/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0244 - val_loss: 0.0764\n",
            "Epoch 5/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0185 - val_loss: 0.0762\n",
            "Epoch 6/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0148 - val_loss: 0.0770\n",
            "1386\n",
            "0.0005\n",
            "Epoch 7/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0147 - val_loss: 0.0803\n",
            "Epoch 8/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0110 - val_loss: 0.0786\n",
            "Epoch 9/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0085 - val_loss: 0.0816\n",
            "Epoch 10/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0072 - val_loss: 0.0848\n",
            "Epoch 11/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0065 - val_loss: 0.0831\n",
            "Epoch 12/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0057 - val_loss: 0.0821\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0076 - val_loss: 0.0851\n",
            "Epoch 14/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0069 - val_loss: 0.0906\n",
            "Epoch 15/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0061 - val_loss: 0.0890\n",
            "Epoch 16/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0057 - val_loss: 0.0898\n",
            "Epoch 17/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0050 - val_loss: 0.0886\n",
            "Epoch 18/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0047 - val_loss: 0.0887\n",
            "4158\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.0705\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 13ms/step - loss: 0.0622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r2it [04:02, 124.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "231/231 [==============================] - 6s 18ms/step - loss: 0.1431 - val_loss: 0.0916\n",
            "Epoch 2/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0616 - val_loss: 0.0797\n",
            "Epoch 3/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0365 - val_loss: 0.0769\n",
            "Epoch 4/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0251 - val_loss: 0.0742\n",
            "Epoch 5/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0186 - val_loss: 0.0781\n",
            "Epoch 6/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0152 - val_loss: 0.0774\n",
            "1386\n",
            "0.0005\n",
            "Epoch 7/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0153 - val_loss: 0.0798\n",
            "Epoch 8/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0109 - val_loss: 0.0820\n",
            "Epoch 9/18\n",
            "231/231 [==============================] - 3s 14ms/step - loss: 0.0086 - val_loss: 0.0812\n",
            "Epoch 10/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0075 - val_loss: 0.0825\n",
            "Epoch 11/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0064 - val_loss: 0.0813\n",
            "Epoch 12/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0058 - val_loss: 0.0821\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0074 - val_loss: 0.0867\n",
            "Epoch 14/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0067 - val_loss: 0.0908\n",
            "Epoch 15/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0063 - val_loss: 0.0850\n",
            "Epoch 16/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0060 - val_loss: 0.0831\n",
            "Epoch 17/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0053 - val_loss: 0.0863\n",
            "Epoch 18/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0047 - val_loss: 0.0868\n",
            "4158\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.0682\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.0585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r3it [05:54, 120.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "231/231 [==============================] - 6s 20ms/step - loss: 0.1474 - val_loss: 0.0862\n",
            "Epoch 2/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0629 - val_loss: 0.0739\n",
            "Epoch 3/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0367 - val_loss: 0.0742\n",
            "Epoch 4/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0245 - val_loss: 0.0735\n",
            "Epoch 5/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0186 - val_loss: 0.0704\n",
            "Epoch 6/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0151 - val_loss: 0.0716\n",
            "1386\n",
            "0.0005\n",
            "Epoch 7/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0149 - val_loss: 0.0797\n",
            "Epoch 8/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0107 - val_loss: 0.0740\n",
            "Epoch 9/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0082 - val_loss: 0.0755\n",
            "Epoch 10/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0075 - val_loss: 0.0747\n",
            "Epoch 11/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0067 - val_loss: 0.0739\n",
            "Epoch 12/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0059 - val_loss: 0.0785\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0068 - val_loss: 0.0791\n",
            "Epoch 14/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0062 - val_loss: 0.0774\n",
            "Epoch 15/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0061 - val_loss: 0.0789\n",
            "Epoch 16/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0054 - val_loss: 0.0819\n",
            "Epoch 17/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0050 - val_loss: 0.0820\n",
            "Epoch 18/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0045 - val_loss: 0.0807\n",
            "4158\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 13ms/step - loss: 0.0664\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 13ms/step - loss: 0.0582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r4it [07:49, 119.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            "231/231 [==============================] - 6s 20ms/step - loss: 0.1476 - val_loss: 0.0931\n",
            "Epoch 2/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0618 - val_loss: 0.0875\n",
            "Epoch 3/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0366 - val_loss: 0.0937\n",
            "Epoch 4/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0256 - val_loss: 0.0853\n",
            "Epoch 5/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0191 - val_loss: 0.0875\n",
            "Epoch 6/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0157 - val_loss: 0.0887\n",
            "1386\n",
            "0.0005\n",
            "Epoch 7/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0155 - val_loss: 0.0982\n",
            "Epoch 8/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0111 - val_loss: 0.0964\n",
            "Epoch 9/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0089 - val_loss: 0.1043\n",
            "Epoch 10/18\n",
            "231/231 [==============================] - 3s 15ms/step - loss: 0.0076 - val_loss: 0.1001\n",
            "Epoch 11/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0065 - val_loss: 0.1050\n",
            "Epoch 12/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0061 - val_loss: 0.1036\n",
            "2772\n",
            "0.0005\n",
            "Epoch 13/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0068 - val_loss: 0.1080\n",
            "Epoch 14/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0073 - val_loss: 0.1081\n",
            "Epoch 15/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0062 - val_loss: 0.1278\n",
            "Epoch 16/18\n",
            "231/231 [==============================] - 4s 16ms/step - loss: 0.0058 - val_loss: 0.1263\n",
            "Epoch 17/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0051 - val_loss: 0.1124\n",
            "Epoch 18/18\n",
            "231/231 [==============================] - 4s 15ms/step - loss: 0.0050 - val_loss: 0.1210\n",
            "4158\n",
            "0.0005\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 13ms/step - loss: 0.0713\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 1s 13ms/step - loss: 0.0621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5it [09:45, 117.09s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8652f5d3-6d9c-4c91-cbfa-a2e72622c896"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre.ravel()/10\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.014690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.001794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.001412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.000011\n",
              "3663550        0.000009\n",
              "3663551        0.014690\n",
              "3663552        0.001794\n",
              "3663553        0.001412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo"
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "baeb7970-a3a0-4e29-fc43-35c7e02f2534"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre/10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faef9a74048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeWMkwJIWng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}